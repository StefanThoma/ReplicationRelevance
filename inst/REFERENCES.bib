
@article{ahn2012,
  title = {Use of the Estimated Intraclass Correlation for Correcting Differences in Effect Size by Level},
  author = {Ahn, Soyeon and Myers, Nicolas D. and Jin, Ying},
  year = {2012},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {44},
  number = {2},
  pages = {490--502},
  issn = {1554-3528},
  doi = {10.3758/s13428-011-0153-1},
  abstract = {In a meta-analysis of intervention or group comparison studies, researchers often encounter the circumstance in which the standardized mean differences (d-effect sizes) are computed at multiple levels (e.g., individual vs. cluster). Cluster-level d-effect sizes may be inflated and, thus, may need to be corrected using the intraclass correlation (ICC) before being combined with individual-level d-effect sizes. The ICC value, however, is seldom reported in primary studies and, thus, may need to be computed from other sources. This article proposes a method for estimating the ICC value from the reported standard deviations within a particular meta-analysis (i.e., estimated ICC) when an appropriate default ICC value (Hedges, 2009b) is unavailable. A series of simulations provided evidence that the proposed method yields an accurate and precise estimated ICC value, which can then be used for correct estimation of a d-effect size. The effects of other pertinent factors (e.g., number of studies) were also examined, followed by discussion of related limitations and future research in this area.},
  language = {en}
}

@article{albarracin2008,
  title = {Increasing and Decreasing Motor and Cognitive Output: {{A}} Model of General Action and Inaction Goals},
  shorttitle = {Increasing and Decreasing Motor and Cognitive Output},
  author = {Albarrac{\'i}n, Dolores and Handley, Ian M. and Noguchi, Kenji and McCulloch, Kathleen C. and Li, Hong and Leeper, Joshua and Brown, Rick D. and Earl, Allison and Hart, William P.},
  year = {2008},
  journal = {Journal of Personality and Social Psychology},
  volume = {95},
  number = {3},
  pages = {510--523},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/a0012833},
  abstract = {General action and inaction goals can influence the amount of motor or cognitive output irrespective of the type of behavior in question, with the same stimuli producing trivial and important motor and cognitive manifestations normally viewed as parts of different systems. A series of experiments examined the effects of instilling general action and inaction goals using word primes, such as "action" and "rest." The first 5 experiments showed that the same stimuli influenced motor output, such as doodling on a piece of paper and eating, as well as cognitive output, such as recall and problem solving. The last 2 experiments supported the prediction that these diverse effects can result from the instigation of general action and inaction goals. Specifically, these last 2 studies confirmed that participants were motivated to achieve active or inactive states and that attaining them decreased the effects of the primes on behavior. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Ability,Physical Activity,Problem Solving,Self-Regulation}
}

@book{altman2013,
  title = {Statistics with {{Confidence}}: {{Confidence Intervals}} and {{Statistical Guidelines}}},
  shorttitle = {Statistics with {{Confidence}}},
  author = {Altman, Douglas and Machin, David and Bryant, Trevor and Gardner, Martin},
  year = {2013},
  month = jun,
  publisher = {{John Wiley \& Sons}},
  abstract = {This highly popular introduction to confidence intervals has been thoroughly updated and expanded. It includes methods for using confidence intervals, with illustrative worked examples and extensive guidelines and checklists to help the novice.},
  googlebooks = {HmnIBAAAQBAJ},
  isbn = {978-1-118-70250-5},
  language = {en},
  keywords = {Medical / Biostatistics,Medical / General}
}

@article{anderson2016,
  title = {There's More than One Way to Conduct a Replication Study: {{Beyond}} Statistical Significance},
  shorttitle = {There's More than One Way to Conduct a Replication Study},
  author = {Anderson, Samantha F. and Maxwell, Scott E.},
  year = {2016},
  journal = {Psychological Methods},
  volume = {21},
  number = {1},
  pages = {1--12},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000051},
  abstract = {As the field of psychology struggles to trust published findings, replication research has begun to become more of a priority to both scientists and journals. With this increasing emphasis placed on reproducibility, it is essential that replication studies be capable of advancing the field. However, we argue that many researchers have been only narrowly interpreting the meaning of replication, with studies being designed with a simple statistically significant or nonsignificant results framework in mind. Although this interpretation may be desirable in some cases, we develop a variety of additional ``replication goals'' that researchers could consider when planning studies. Even if researchers are aware of these goals, we show that they are rarely used in practice\textemdash as results are typically analyzed in a manner only appropriate to a simple significance test. We discuss each goal conceptually, explain appropriate analysis procedures, and provide 1 or more examples to illustrate these analyses in practice. We hope that these various goals will allow researchers to develop a more nuanced understanding of replication that can be flexible enough to answer the various questions that researchers might seek to understand. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Effect Size (Statistical),Experimental Replication,Statistical Analysis,Statistical Significance}
}

@article{blume2019,
  title = {An {{Introduction}} to {{Second}}-{{Generation}} p-{{Values}}},
  author = {Blume, Jeffrey D. and Greevy, Robert A. and Welty, Valerie F. and Smith, Jeffrey R. and Dupont, William D.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {157--167},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537893},
  abstract = {Second generation p-values preserve the simplicity that has made p-values popular while resolving critical flaws that promote misinterpretation of data, distraction by trivial effects, and unreproducible assessments of data. The second-generation p-value (SGPV) is an extension that formally accounts for scientific relevance by using a composite null hypothesis that captures null and scientifically trivial effects. Because the majority of spurious findings are small effects that are technically nonnull but practically indistinguishable from the null, the second-generation approach greatly reduces the likelihood of a false discovery. SGPVs promote transparency, rigor and reproducibility of scientific results by a priori identifying which candidate hypotheses are practically meaningful and by providing a more reliable statistical summary of when the data are compatible with the candidate hypotheses or null hypotheses, or when the data are inconclusive. We illustrate the importance of these advances using a dataset of 247,000 single-nucleotide polymorphisms, i.e., genetic markers that are potentially associated with prostate cancer.},
  keywords = {Likelihood ratios,Null hypothesis,p-Value,Statistical evidence},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1537893}
}

@article{bonett2021,
  title = {Design and {{Analysis}} of {{Replication Studies}}},
  author = {Bonett, Douglas G.},
  year = {2021},
  month = jul,
  journal = {Organizational Research Methods},
  volume = {24},
  number = {3},
  pages = {513--529},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428120911088},
  abstract = {Issues surrounding the importance and interpretation of replication research have generated considerable debate and controversy in recent years. Some of the controversy can be attributed to imprecise and inadequate specifications of the statistical criteria needed to assess replication and nonreplication. Two types of statistical replication evidence and four types of statistical nonreplication evidence are described. In addition, three types of inconclusive statistical replication evidence are described. An important benefit of a replication study is the ability to combine an effect-size estimate from the original study with an effect-size estimate from the follow-up study to obtain a more precise and generalizable effect-size estimate. The sample size in the follow-up study is an important design consideration, and some methods for determining the follow-up sample size requirements are discussed. R functions are provided that can be used to analyze results from a replication study. R functions to determine the appropriate sample size in the follow-up study also are provided.},
  language = {en},
  keywords = {confidence intervals,directional tests,equivalence tests,inconclusive replication,nonreplication evidence,replication evidence,sample size requirements}
}

@book{borenstein2009,
  title = {Introduction to Meta-Analysis},
  author = {Borenstein, Michael},
  year = {2009},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester, West Sussex, U.K. ;}},
  abstract = {This book provides a clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies. Meta-analysis has become a critically important tool in fields as diverse as medicine, pharmacology, epidemiology, education, psychology, business, and ecology. Introduction to Meta-Analysis:Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in},
  collaborator = {Hedges, Larry V. and Higgins, Julian P. T. and Rothstein, Hannah R.},
  isbn = {978-1-119-96437-7},
  language = {eng},
  keywords = {Meta-analysis}
}

@article{brysbaert2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  abstract = {Article: Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  language = {en}
}

@article{collaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Collaboration, Open Science},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  language = {en},
  pmid = {26315443}
}

@article{colquhoun2017,
  title = {The Reproducibility of Research and the Misinterpretation of P-Values},
  author = {Colquhoun, David},
  year = {2017},
  journal = {Royal Society Open Science},
  volume = {4},
  number = {12},
  pages = {171085},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.171085},
  abstract = {We wish to answer this question: If you observe a `significant' p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe p\,=\,0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3\,:\,1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the p-value. And if you want to limit the false positive risk to 5\%, you would have to assume that you were 87\% sure that there was a real effect before the experiment was done. If you observe p\,=\,0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100\,:\,1 odds on there being a real effect. That would usually be regarded as conclusive. But the false positive risk would still be 8\% if the prior probability of a real effect were only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5\% you would need to observe p\,=\,0.00045. It is recommended that the terms `significant' and `non-significant' should never be used. Rather, p-values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5\%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed p-value. Despite decades of warnings, many areas of science still insist on labelling a result of p\,{$<$}\,0.05 as `statistically significant'. This practice must contribute to the lack of reproducibility in some areas of science. This is before you get to the many other well-known problems, like multiple comparisons, lack of randomization and p-hacking. Precise inductive inference is impossible and replication is the only way to be sure. Science is endangered by statistical misunderstanding, and by senior people who impose perverse incentives on scientists.}
}

@misc{corker2020,
  title = {Many {{Labs}} 5: {{Registered Replication}} of {{Albarrac\'in}} et al. (2008), {{Experiment}} 7},
  shorttitle = {Many {{Labs}} 5},
  author = {Corker, Katherine S. and Arnal, Jack and Bonfiglio, Diane B. V. and Curran, Paul G. and Chartier, Christopher R. and Chopik, William J. and Guadagno, Rosanna and Kimbrough, Amanda and Schmidt, Kathleen and Wiggins, Bradford J.},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/qzspr},
  abstract = {Albarrac\'in et al. (2008, Study 7) tested whether priming action or inaction goals (vs. no goal) and then satisfying those goals (vs. not satisfying them) would be associated with subsequent cognitive responding. They hypothesized and found that priming action or inaction goals that were not satisfied resulted in greater or lesser responding, respectively, compared with not priming goals (N= 98). Sonnleitner and Voracek (2015) attempted to directly replicate Albarrac\'in et al.'s (2008) study with German participants (N= 105). They did not find evidence for the 3x2 interaction or the expected main effect of task type. The current study attempted to directly replicate Albarrac\'in et al. (2008), Study 7, with a larger sample of participants (N=1,690) from seven colleges and universities in the United States. We also extended the study design by using a scrambled-sentence task to prime goals instead of the original task of completing word fragments, allowing us to test whether study protocol moderated any effects of interest. We did not detect moderation by protocol in the full 3x2x2 design (pseudo-r2=0.05\%). Results for both protocols were largely consistent with Sonnleitner and Voracek's findings (pseudo-r2s = 0.14\% and 0.50\%). We consider these results in light of recent findings concerning priming methods and discuss the robustness of action-/inaction-goal priming to the implementation of different protocols in this particular context.},
  keywords = {Many Labs,ML5,Replication,RPP,Social and Behavioral Sciences,Social and Personality Psychology,Social Cognition}
}

@article{cumming2008,
  title = {Replication and p {{Intervals}}: P {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  shorttitle = {Replication and p {{Intervals}}},
  author = {Cumming, Geoff},
  year = {2008},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {3},
  number = {4},
  pages = {286--300},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  abstract = {Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is ``Surprisingly little.'' In one simulation of 25 repetitions of a typical experiment, p varied from {$<$}.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p {$<$}.00008, and fully a 10\% chance that p {$>$}.44. Remarkably, the interval\textemdash termed a p interval\textemdash is this wide however large the sample size. p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and model-fitting techniques and by adopting meta-analytic thinking.},
  language = {en},
  file = {/Users/st/Zotero/storage/FKYEUTV4/Cumming - 2008 - Replication and p Intervals p Values Predict the .pdf}
}

@article{diener2019,
  title = {3.7 {{The Replication Crisis}} in {{Psychology}}},
  author = {Diener, Edward and {Biswas-Diener}, Robert},
  year = {2019},
  month = jun,
  journal = {Introduction to Psychology},
  publisher = {{University of Saskatchewan Open Press}},
  language = {en},
  file = {/Users/st/Zotero/storage/YEH4I688/the-replication-crisis-in-psychology.html}
}

@article{ebersole2016,
  title = {Many Labs 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many Labs 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J.N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  journal = {Journal of Experimental Social Psychology},
  volume = {67},
  pages = {68--82},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1096-0465(Electronic),0022-1031(Print)},
  doi = {10.1016/j.jesp.2015.10.012},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences\textemdash conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Experimental Replication,Individual Differences,Sampling (Experimental),Social Psychology},
  file = {/Users/st/Zotero/storage/YTMNDBB5/Ebersole et al. - 2016 - Many labs 3 Evaluating participant pool quality a.pdf;/Users/st/Zotero/storage/9CKX49SR/2016-46348-013.html}
}

@article{ebersole2020,
  title = {Many {{Labs}} 5: {{Testing Pre}}-{{Data}}-{{Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and {Bart-Plange}, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarevi{\'c}, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban{\'i}k, Gabriel and Baskin, Ernest and Belopavlovi{\'c}, Radomir and Bernstein, Michael H. and Bia{\l}ek, Micha{\l} and Bloxsom, Nicholas G. and Bodro{\v z}a, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br{\"u}hlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and {\v C}oli{\'c}, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc{\~a}o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M{\'a}ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha{\l}asa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H{\"u}ffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and {Jim{\'e}nez-Leal}, William and Johannesson, Magnus and {Joy-Gaba}, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko{\l}odziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and {de Lima}, Tiago Jess{\'e} Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, {\L}ukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa{\l} and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orli{\'c}, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedovi{\'c}, Ivana and P{\k{e}}kala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrovi{\'c}, Boban and Pfeiffer, Thomas and Pie{\'n}kosz, Damian and Preti, Emanuele and Puri{\'c}, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys{\l}aw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and {Schulz-Hardt}, Stefan and Sch{\"u}tz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R{\'u}ben and Sioma, Barbara and Skorb, Lauren and {de Souza}, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilovi{\'c}, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz{\"o}ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and {\v Z}e{\v z}elj, Iris and Zrubka, Mark and Nosek, Brian A.},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3\textendash 9; median total sample = 1,279.5, range = 276\textendash 3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols ({$\Delta$}r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00\textendash.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19\textendash.50).},
  language = {en},
  keywords = {metascience,open data,peer review,preregistered,Registered Reports,replication,reproducibility}
}

@incollection{feldman,
  title = {{{HKU PSYC2020 Fundamentals}} of {{Social Psychology}}},
  booktitle = {{{HKU PSYC2020 Fundamentals}} of {{Social Psychology}}},
  author = {Feldman, Gilad},
  pages = {192},
  language = {en},
  file = {/Users/st/Zotero/storage/LZI4ZE5Y/Feldman - HKU PSYC2020 Fundamentals of Social Psychology.pdf}
}

@article{fletcher2021,
  title = {How (Not) to Measure Replication},
  author = {Fletcher, Samuel C.},
  year = {2021},
  month = jun,
  journal = {European Journal for Philosophy of Science},
  volume = {11},
  number = {2},
  pages = {57},
  issn = {1879-4920},
  doi = {10.1007/s13194-021-00377-2},
  abstract = {The replicability crisis refers to the apparent failures to replicate both important and typical positive experimental claims in psychological science and biomedicine, failures which have gained increasing attention in the past decade. In order to provide evidence that there is a replicability crisis in the first place, scientists have developed various measures of replication that help quantify or ``count'' whether one study replicates another. In this nontechnical essay, I critically examine five types of replication measures used in the landmark article ``Estimating the reproducibility of psychological science'' (Open Science Collaboration, Science, 349, ac4716, 2015) based on the following techniques: subjective assessment, null hypothesis significance testing, comparing effect sizes, comparing the original effect size with the replication confidence interval, and meta-analysis. The first four, I argue, remain unsatisfactory for a variety of conceptual or formal reasons, even taking into account various improvements. By contrast, at least one version of the meta-analytic measure does not suffer from these problems. It differs from the others in rejecting dichotomous conclusions, the assumption that one study replicates another or not simpliciter. I defend it from other recent criticisms, concluding however that it is not a panacea for all the multifarious problems that the crisis has highlighted.},
  language = {en}
}

@article{francis2020,
  title = {Excess Success in ``{{Don}}'t Count Calorie Labeling out: {{Calorie}} Counts on the Left Side of Menu Items Lead to Lower Calorie Food Choices''},
  shorttitle = {Excess Success in ``{{Don}}'t Count Calorie Labeling out},
  author = {Francis, Gregory and Thunell, Evelina},
  year = {2020},
  month = aug,
  journal = {Meta-Psychology},
  volume = {4},
  issn = {2003-2714},
  doi = {10.15626/MP.2019.2266},
  copyright = {Copyright (c) 2020 Evelina Thunell, Gregory Francis},
  language = {en},
  keywords = {Consumer Psychology}
}

@article{gibson2021,
  title = {The {{Role}} of P-{{Values}} in {{Judging}} the {{Strength}} of {{Evidence}} and {{Realistic Replication Expectations}}},
  author = {Gibson, Eric W.},
  year = {2021},
  month = jan,
  journal = {Statistics in Biopharmaceutical Research},
  volume = {13},
  number = {1},
  pages = {6--18},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/19466315.2020.1724560},
  abstract = {p-Values are viewed by many as the root cause of the so-called replication crisis, which is characterized by the prevalence of positive scientific findings that are contradicted in subsequent studies. The spectrum of proposed solutions includes redefining statistical significance, abandoning the concept of statistical significance, or eliminating the use of p-values altogether. The unintended consequence of these proposals has been confusion within the scientific community, especially in the absence of consensus or clear alternatives. The goal of this article is to reframe the perceived replication crisis. I argue that this crisis is to a large extent the result of excessive optimism based on unknowingly (and sometimes knowingly) overstated evidence. As a remedy, I suggest a four-part guide to navigating statistical inference with p-values that is accessible for scientists. Examples taken from pharmaceutical drug development for heart failure illustrate key concepts.},
  keywords = {False discovery rate,Multiplicity,Selective inference,Significance test},
  annotation = {\_eprint: https://doi.org/10.1080/19466315.2020.1724560},
  file = {/Users/st/Zotero/storage/WP3HUGI8/Gibson - 2021 - The Role of p-Values in Judging the Strength of Ev.pdf;/Users/st/Zotero/storage/A7CNGJYC/19466315.2020.html}
}

@article{goodman2001,
  title = {Of {{P}}-{{Values}} and {{Bayes}}: {{A Modest Proposal}}},
  shorttitle = {Of {{P}}-{{Values}} and {{Bayes}}},
  author = {Goodman, Steven N.},
  year = {2001},
  month = may,
  journal = {Epidemiology},
  volume = {12},
  number = {3},
  pages = {295--297},
  issn = {1044-3983},
  abstract = {An abstract is unavailable.},
  language = {en-US},
  file = {/Users/st/Zotero/storage/LXRG583Q/of_p_values_and_bayes__a_modest_proposal.6.html}
}

@article{halsey2019,
  title = {The Reign of the P-Value Is over: What Alternative Analyses Could We Employ to Fill the Power Vacuum?},
  shorttitle = {The Reign of the P-Value Is Over},
  author = {Halsey, Lewis G.},
  year = {2019},
  month = may,
  journal = {Biology Letters},
  volume = {15},
  number = {5},
  pages = {20190174},
  publisher = {{Royal Society}},
  doi = {10.1098/rsbl.2019.0174},
  abstract = {The p-value has long been the figurehead of statistical analysis in biology, but its position is under threat. p is now widely recognized as providing quite limited information about our data, and as being easily misinterpreted. Many biologists are aware of p's frailties, but less clear about how they might change the way they analyse their data in response. This article highlights and summarizes four broad statistical approaches that augment or replace the p-value, and that are relatively straightforward to apply. First, you can augment your p-value with information about how confident you are in it, how likely it is that you will get a similar p-value in a replicate study, or the probability that a statistically significant finding is in fact a false positive. Second, you can enhance the information provided by frequentist statistics with a focus on effect sizes and a quantified confidence that those effect sizes are accurate. Third, you can augment or substitute p-values with the Bayes factor to inform on the relative levels of evidence for the null and alternative hypotheses; this approach is particularly appropriate for studies where you wish to keep collecting data until clear evidence for or against your hypothesis has accrued. Finally, specifically where you are using multiple variables to predict an outcome through model building, Akaike information criteria can take the place of the p-value, providing quantified information on what model is best. Hopefully, this quick-and-easy guide to some simple yet powerful statistical options will support biologists in adopting new approaches where they feel that the p-value alone is not doing their data justice.},
  keywords = {AIC,Bayesian,confidence intervals,effect size,statistical analysis},
  file = {/Users/st/Zotero/storage/RB2XZLMJ/Halsey - 2019 - The reign of the p-value is over what alternative.pdf}
}

@article{head2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {PLOS Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  language = {en},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,Publication ethics,Reproducibility,Statistical data,Test statistics},
  file = {/Users/st/Zotero/storage/YB2A5MTH/Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf;/Users/st/Zotero/storage/T6II6V7A/article.html}
}

@article{held2019,
  title = {The Assessment of Intrinsic Credibility and a New Argument for p {$<$} 0.005},
  author = {Held, Leonhard},
  year = {2019},
  journal = {Royal Society Open Science},
  volume = {6},
  number = {3},
  pages = {181534},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.181534},
  abstract = {The concept of intrinsic credibility has been recently introduced to check the credibility of `out of the blue' findings without any prior support. A significant result is deemed intrinsically credible if it is in conflict with a sceptical prior derived from the very same data that would make the effect just non-significant. In this paper, I propose to use Bayesian prior-predictive tail probabilities to assess intrinsic credibility. For the standard 5\% significance level, this leads to a new p-value threshold that is remarkably close to the recently proposed p {$<$} 0.005 standard. I also introduce the credibility ratio, the ratio of the upper to the lower limit (or vice versa) of a confidence interval for a significant effect size. I show that the credibility ratio has to be smaller than 5.8 such that a significant finding is also intrinsically credible. Finally, a p-value for intrinsic credibility is proposed that is a simple function of the ordinary p-value and has a direct frequentist interpretation in terms of the probability of replicating an effect. An application to data from the Open Science Collaboration study on the reproducibility of psychological science suggests that intrinsic credibility of the original experiment is better suited to predict the success of a replication experiment than standard significance.}
}

@article{held2020,
  title = {A New Standard for the Analysis and Design of Replication Studies},
  author = {Held, Leonhard},
  year = {2020},
  month = feb,
  journal = {Journal of the Royal Statistical Society: Series A},
  volume = {183},
  number = {2},
  pages = {431--448},
  publisher = {{Wiley-Blackwell Publishing, Inc.}},
  issn = {0964-1998},
  doi = {10.1111/rssa.12493},
  abstract = {A new standard is proposed for the evidential assessment of replication studies. The approach combines a specific reverse Bayes technique with prior-predictive tail probabilities to define replication success. The method gives rise to a quantitative measure for replication success, called the sceptical p-value. The sceptical p-value integrates traditional significance of both the original and the replication study with a comparison of the respective effect sizes. It incorporates the uncertainty of both the original and the replication effect estimates and reduces to the ordinary p-value of the replication study if the uncertainty of the original effect estimate is ignored. The framework proposed can also be used to determine the power or the required replication sample size to achieve replication success. Numerical calculations highlight the difficulty of achieving replication success if the evidence from the original study is only suggestive. An application to data from the Open Science Collaboration project on the replicability of psychological science illustrates the methodology proposed.},
  copyright = {info:eu-repo/semantics/openAccess},
  language = {eng}
}

@phdthesis{herger2018,
  type = {Seminar {{Work}}},
  title = {Replicability of {{Psychological Studies}}: A {{New Analysis}} of the {{ManyLabs Data Set}}},
  author = {Herger, Lorenz and Rogai, Federico},
  year = {2018},
  month = jun,
  language = {en},
  school = {ETH Zurich}
}

@article{hoogeveen2020,
  title = {Laypeople {{Can Predict Which Social}}-{{Science Studies Will Be Replicated Successfully}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {267--285},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920919667},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here, we assess the extent to which a finding's replication success relates to its intuitive plausibility. Each of 27 high-profile social-science findings was evaluated by 233 people without a Ph.D. in psychology. Results showed that these laypeople predicted replication success with above-chance accuracy (i.e., 59\%). In addition, when participants were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67\%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople's predictions contain useful information for assessing the probability that a given finding will be replicated successfully.},
  language = {en},
  keywords = {meta-science,open data,open materials,open science,prediction survey,preregistered,replication crisis}
}

@article{hutton2019,
  title = {Discussion on the Meeting on `Signs and Sizes:Understanding and Replicating Statistical Findings'},
  shorttitle = {Discussion on the Meeting on `signs and Sizes},
  author = {Hutton, Jane L. and Diggle, Peter John and Bird, Sheila M. and Hennig, Christian and Longford, Nick and Mathur, Maya B. and VanderWeele, Tyler J. and Ioannidis, John P. A. and Chai, Christine P. and Dowe, David L. and Ferguson, John and {Fitz-Simon}, Nicola and Friede, Tim and Rover, Christian and Grieve, A. P. and Kumar, Kuldeep and Ly, Alexander and Mansmann, Ulrich and Mateu, Jorge and Matthews, Robert A. J. and Neuenschwander, Beat and Zwahlen, Marcel and Pericchi, Luis and Roes, Kit C. B. and Senn, Stephen and Wagenmakers, Eric-Jan and Ly, Alexander and Rice, Kenneth and Krakauer, Chloe and Bonnett, Tyler and Held, Leonhard},
  year = {2019},
  month = dec,
  journal = {Journal of the Royal Statistical Society Series A-Statistics in Society},
  volume = {183},
  number = {2},
  pages = {449--469},
  publisher = {{Wiley-Blackwell}},
  issn = {0964-1998},
  doi = {10.1111/rssa.12544},
  language = {English}
}

@article{ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/Users/st/Zotero/storage/N6D7E2W5/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf;/Users/st/Zotero/storage/5BAIN9ZM/article.html}
}

@article{kenny2019,
  title = {The Unappreciated Heterogeneity of Effect Sizes: {{Implications}} for Power, Precision, Planning of Research, and Replication},
  shorttitle = {The Unappreciated Heterogeneity of Effect Sizes},
  author = {Kenny, David A. and Judd, Charles M.},
  year = {2019},
  journal = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {578--589},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000209},
  abstract = {Repeated investigations of the same phenomenon typically yield effect sizes that vary more than one would expect from sampling error alone. Such variation is even found in exact replication studies, suggesting that it is not only because of identifiable moderators but also to subtler random variation across studies. Such heterogeneity of effect sizes is typically ignored, with unfortunate consequences. We consider its implications for power analyses, the precision of estimated effects, and the planning of original and replication research. With heterogeneity and an interest in generalizing to a population of studies, the usual power calculations and confidence intervals are likely misleading, and the preference for single definitive large-N studies is misguided. Researchers and methodologists need to recognize that effects are often heterogeneous and plan accordingly. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Confidence Limits (Statistics),Experimental Replication,Population (Statistics),Prediction,Statistical Power}
}

@article{kerr1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  publisher = {{SAGE Publications Inc}},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  language = {en},
  file = {/Users/st/Zotero/storage/8B7SNSDN/s15327957pspr0203_4.pdf;/Users/st/Zotero/storage/YIQ8TEQY/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf}
}

@article{klein2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  month = jan,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  publisher = {{Hogrefe Publishing}},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.}
}

@article{klein2018a,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  language = {en},
  keywords = {cognitive psychology,culture,individual differences,meta-analysis,open data,open materials,preregistered,Registered Report,replication,sampling effects,situational effects,social psychology}
}

@misc{klein2019,
  title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
  shorttitle = {Many {{Labs}} 4},
  author = {Klein, Richard A. and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret and Cromar, Ryan and Vidamuerte, DeVere and Gardiner, Gwendolyn and Gosnell, Courtney and Grahe, Jon and Hall, Calvin and {Joy-Gaba}, Jennifer and Legg, Angela M. and Levitan, Carmel and Mancini, Anthony and Manfredi, Dylan and Miller, Jason Michael and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine and Storage, Daniel and Swanson, Trevor and van Swol, Lyn and Vaughn, Leigh Ann and Ratliff, Kate},
  year = {2019},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/vef2c},
  abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 21 Labs and N = 2,220 participants) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not yet understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
  keywords = {many labs,Meta-science,metascience,mortality salience,psychology,replication,Social and Behavioral Sciences,Social and Personality Psychology,terror management theory},
  file = {/Users/st/Zotero/storage/FTASQ8TN/Klein et al. - 2019 - Many Labs 4 Failure to Replicate Mortality Salien.pdf}
}

@article{kvarven2020,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Str{\o}mland, Eirik and Johannesson, Magnus},
  year = {2020},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {4},
  pages = {423--434},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15\,meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15\,meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en}
}

@article{lakens2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Dani{\"e}l},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {355--362},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  language = {en},
  keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods}
}

@article{lobue2008,
  title = {Detecting the {{Snake}} in the {{Grass}}: {{Attention}} to {{Fear}}-{{Relevant Stimuli}} by {{Adults}} and {{Young Children}}},
  shorttitle = {Detecting the {{Snake}} in the {{Grass}}},
  author = {LoBue, Vanessa and DeLoache, Judy S.},
  year = {2008},
  month = mar,
  journal = {Psychological Science},
  volume = {19},
  number = {3},
  pages = {284--289},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2008.02081.x},
  abstract = {Snakes are among the most common targets of fears and phobias. In visual detection tasks, adults detect their presence more rapidly than the presence of other kinds of visual stimuli. We report evidence that very young children share this attentional bias. In three experiments, preschool children and adults were asked to find a single target picture among an array of eight distractors. Both the children and the adults detected snakes more rapidly than three types of nonthreatening stimuli (flowers, frogs, and caterpillars). These results provide the first evidence of enhanced visual detection of evolutionarily relevant threat stimuli in young children.},
  language = {en}
}

@article{martin2017,
  title = {Are {{Psychology Journals Anti}}-Replication? {{A Snapshot}} of {{Editorial Practices}}},
  shorttitle = {Are {{Psychology Journals Anti}}-Replication?},
  author = {Martin, G. N. and Clarke, Richard M.},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {0},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00523},
  abstract = {Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias \textendash{} the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3\%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.},
  language = {English},
  keywords = {JOURNAL EDITORIAL PRACTICES,p-hacking,Psychology,Publication Bias,Replication},
  file = {/Users/st/Zotero/storage/TAINS6UZ/Martin and Clarke - 2017 - Are Psychology Journals Anti-replication A Snapsh.pdf}
}

@article{maxwell2015,
  title = {Is Psychology Suffering from a Replication Crisis? {{What}} Does ``Failure to Replicate'' Really Mean?},
  shorttitle = {Is Psychology Suffering from a Replication Crisis?},
  author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
  year = {2015},
  journal = {American Psychologist},
  volume = {70},
  number = {6},
  pages = {487--498},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/a0039400},
  abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and meta-analysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology's alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Replication,Failure,Statistical Power,Statistical Probability},
  file = {/Users/st/Zotero/storage/G57Q8IDE/Maxwell et al. - 2015 - Is psychology suffering from a replication crisis.pdf;/Users/st/Zotero/storage/I9M9YD98/2015-39598-001.html}
}

@article{meehl1967,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  year = {1967},
  month = jun,
  journal = {Philosophy of Science},
  volume = {34},
  number = {2},
  pages = {103--115},
  publisher = {{The University of Chicago Press}},
  issn = {0031-8248},
  doi = {10.1086/288135},
  abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation.}
}

@article{nuzzo2014,
  title = {Scientific Method: {{Statistical}} Errors},
  shorttitle = {Scientific Method},
  author = {Nuzzo, Regina},
  year = {2014},
  month = feb,
  journal = {Nature},
  volume = {506},
  number = {7487},
  pages = {150--152},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/506150a},
  abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
  copyright = {2014 Nature Publishing Group},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: News Subject\_term: Lab life;Mathematics and computing;Medical research;Publishing Subject\_term\_id: lab-life;mathematics-and-computing;medical-research;publishing},
  file = {/Users/st/Zotero/storage/R29APKF7/Nuzzo - 2014 - Scientific method Statistical errors.pdf;/Users/st/Zotero/storage/ZJXZUGLJ/506150a.html}
}

@article{payne2008,
  title = {Why Do Implicit and Explicit Attitude Tests Diverge? {{The}} Role of Structural Fit},
  shorttitle = {Why Do Implicit and Explicit Attitude Tests Diverge?},
  author = {Payne, B. Keith and Burkley, Melissa A. and Stokes, Mark B.},
  year = {2008},
  journal = {Journal of Personality and Social Psychology},
  volume = {94},
  number = {1},
  pages = {16--31},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/0022-3514.94.1.16},
  abstract = {Implicit and explicit attitude tests are often weakly correlated, leading some theorists to conclude that implicit and explicit cognition are independent. Popular implicit and explicit tests, however, differ in many ways beyond implicit and explicit cognition. The authors examined in 4 studies whether correlations between implicit and explicit tests were influenced by the similarity in task demands (i.e., structural fit) and, hence, the processes engaged by each test. Using an affect misattribution procedure, they systematically varied the structural fit of implicit and explicit tests of racial attitudes. As test formats became more similar, the implicit-explicit correlation increased until it became higher than in most previous research. When tests differ in structure, they may underestimate the relationship between implicit and explicit cognition. The authors propose a solution that uses procedures to maximize structural fit. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Attitude Measures,Prejudice,Racial and Ethnic Attitudes,Social Cognition}
}

@article{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors},
  file = {/Users/st/Zotero/storage/4QN4Y7F3/Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf;/Users/st/Zotero/storage/MWM4JW25/1979-27602-001.html}
}

@article{schwarz1985,
  title = {Response {{Scales}}: {{Effects}} of {{Category Range}} on {{Reported Behavior}} and {{Comparative Judgments}}},
  shorttitle = {Response {{Scales}}},
  author = {SCHWARZ, NORBERT and HIPPLER, HANS-J. and DEUTSCH, BRIGITTE and STRACK, FRITZ},
  year = {1985},
  month = jan,
  journal = {Public Opinion Quarterly},
  volume = {49},
  number = {3},
  pages = {388--395},
  issn = {0033-362X},
  doi = {10.1086/268936},
  abstract = {Effects of the range of response categories provided in a closed answer format on behavioral reports and subsequent judgments were explored. Respondents reported their daily use of television along a scale that either ranged from ``up to a half hour'' ``to more than two and a half hours'' or ranged from ``up to two and a half hours'' to ``more than four and a half hours.'' The former subjects reported less use of television than the latter and estimated the average use of TV to be lower. Moreover, the former subjects evaluated TV to be more important in their lives (Experiment 1) and reported less satisfaction with the variety of their leisure-time activities (Experiment 2). These results indicate that subjects inferred the average amount of television watching from the response alternatives provided them and used it as a standard of comparison in evaluating their behavior and its implications.}
}

@article{serra-garcia2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  abstract = {We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published. Published papers that fail to replicate are cited more than those that replicate, even after the failure is published.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. https://creativecommons.org/licenses/by-nc/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  language = {en},
  pmid = {34020944}
}

@article{somethingelse2019,
  title = {Many {{Labs}} 5: {{Registered Replication Report}} of {{LoBue}} \& {{DeLoache}} (2018)},
  author = {Somethingelse and DeLoach},
  year = {2019},
  month = may,
  publisher = {{OSF}},
  abstract = {Hosted on the Open Science Framework},
  language = {en}
}

@article{stahel2021,
  title = {New Relevance and Significance Measures to Replace P-Values},
  author = {Stahel, Werner A.},
  year = {2021},
  journal = {Submitted},
  abstract = {The p-value has been debated exorbitantly in the last decades, experiencing fierce critique, but also finding some advocates. The fundamental issue with its misleading interpretation stems from its common use for testing the unrealistic null hypothesis of an effect that is precisely zero. A meaningful question asks instead whether the effect is relevant. It is then unavoidable that a threshold for relevance is chosen. Considerations that can lead to agreeable conventions for this choice are presented for several commonly used statistical situations. Based on the threshold, a simple quantitative measure of relevance emerges naturally. Statistical inference for the effect should be based on the confidence interval for the relevance measure. A classification of results that goes beyond a simple distinction like ``significant / non-significant'' is proposed. On the other hand, if desired, a single number called the ``secured relevance'' may summarize the result, like the p-value does it, but with a scientifically meaningful interpretation.}
}

@article{stahel2021a,
  title = {Replicability: {{Terminology}}, {{Measuring Success}}, {{Strategy}}},
  author = {Stahel, Werner},
  year = {2021},
  journal = {Unpublished},
  language = {english}
}

@article{stroebe2019,
  title = {What {{Can We Learn}} from {{Many Labs Replications}}?},
  author = {Stroebe, Wolfgang},
  year = {2019},
  month = mar,
  journal = {Basic and Applied Social Psychology},
  volume = {41},
  number = {2},
  pages = {91--103},
  publisher = {{Routledge}},
  issn = {0197-3533},
  doi = {10.1080/01973533.2019.1577736},
  abstract = {Several hundred research groups attempted replications of published effects in so-called Many Labs studies involving thousands of research participants. Given this enormous investment, it seems timely to assess what has been learned and what can be learned from this type of project. My evaluation addresses four questions: First, do these replication studies inform us about the replicability of social psychological research? Second, can replications detect fraud? Third, does the failure to replicate a finding indicate that the original result was wrong? Finally, do these replications help to support or disprove any social psychological theories? Although evidence of replication failures resulted in important methodological changes, the 2015 Open Science Collaboration findings sufficed to make the point. To assess the state of social psychology, we have to evaluate theories rather than randomly selected research findings.},
  annotation = {\_eprint: https://doi.org/10.1080/01973533.2019.1577736},
  file = {/Users/st/Zotero/storage/KZL3KC8M/Stroebe - 2019 - What Can We Learn from Many Labs Replications.pdf;/Users/st/Zotero/storage/FCKTANBX/01973533.2019.html}
}

@article{szucs2016,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John PA},
  year = {2016},
  month = aug,
  journal = {bioRxiv},
  pages = {071530},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/071530},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We have empirically assessed the distribution of published effect sizes and estimated power by extracting more than 100,000 statistical records from about 10,000 cognitive neuroscience and psychology papers published during the past 5 years. The reported median effect size was d=0.93 (inter-quartile range: 0.64-1.46) for nominally statistically significant results and d=0.24 (0.11-0.42) for non-significant results. Median power to detect small, medium and large effects was 0.12, 0.44 and 0.73, reflecting no improvement through the past half-century. Power was lowest for cognitive neuroscience journals. 14\% of papers reported some statistically significant results, although the respective F statistic and degrees of freedom proved that these were non-significant; p value errors positively correlated with journal impact factors. False report probability is likely to exceed 50\% for the whole literature. In light of our findings the recently reported low replication success in psychology is realistic and worse performance may be expected for cognitive neuroscience.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2016, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language = {en},
  file = {/Users/st/Zotero/storage/3B22RV7R/Szucs and Ioannidis - 2016 - Empirical assessment of published effect sizes and.pdf;/Users/st/Zotero/storage/IGRYTQG6/071530v1.html}
}

@article{trafimow2018,
  ids = {trafimow2018a},
  title = {An a Priori Solution to the Replication Crisis},
  author = {Trafimow, David},
  year = {2018},
  month = nov,
  journal = {Philosophical Psychology},
  volume = {31},
  number = {8},
  pages = {1188--1214},
  publisher = {{Routledge}},
  issn = {0951-5089, 1465-394X},
  doi = {10.1080/09515089.2018.1490707},
  abstract = {Possibly, the replication crisis constitutes the most important problem in psychology. It calls into question whether psychology is a science. Existing conceptualizations of replicability depend on effect sizes; the larger the population effect size, the greater the probability of replication. This is problematic and contributes to the replication crisis. A different conceptualization, not dependent on population effect sizes, is desirable. The proposed solution features the closeness of sample means to their corresponding population means, in both the original and replication experiments. If the researcher has specified the sampling precision desired, it is possible to calculate the probability of replication, prior to data collection, and without dependence on the population effect size or expected population effect size. In addition, it is not necessary to know population means or standard deviations, nor sample means or standard deviations, to employ the proposed a priori way of thinking about replicability.},
  language = {en},
  keywords = {A priori procedure,closeness,confidence,precision,probability of replication,replication crisis},
  annotation = {\_eprint: https://doi.org/10.1080/09515089.2018.1490707},
  file = {/Users/st/Zotero/storage/3MXBBNTB/Trafimow - 2018 - An a priori solution to the replication crisis.pdf}
}

@article{tukey1962,
  title = {The {{Future}} of {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1962},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {1},
  pages = {1--67},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177704711},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/st/Zotero/storage/LXJWUK5W/Tukey - 1962 - The Future of Data Analysis.pdf;/Users/st/Zotero/storage/J7VPJYJJ/1177704711.html}
}

@misc{yarkoni2019,
  title = {The {{Generalizability Crisis}}},
  author = {Yarkoni, Tal},
  year = {2019},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jqw35},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned\textemdash that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology\textemdash the linear mixed model\textemdash I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that whereas the "random effect" formalism is used pervasively in psychology to model inter-subject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that the failure to  problems many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  keywords = {generalization,inference,philosophy of science,prediction,psychology,Quantitative Methods,random effects,Social and Behavioral Sciences,statistics,Theory and Philosophy of Science}
}


